{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import PIL\n",
    "from random import shuffle\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataconvall.pt', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "with open('labelsall.pt', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "    \n",
    "with open('datavalconvall.pt', 'rb') as f:\n",
    "    dataVal = pickle.load(f)\n",
    "    \n",
    "with open('labelsval.pt', 'rb') as f:\n",
    "    labelsVal = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(786816,2)    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0),-1)\n",
    "        out = torch.log_softmax(self.linear(x),1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoUXDataset(Dataset):\n",
    "    def __init__(self, data, labels,isTrain=True,isVal=False):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.isTrain = isTrain\n",
    "        self.isVal = isVal\n",
    "        if isVal:\n",
    "            self.testX = torch.from_numpy(np.asarray(data,dtype=np.uint8)).type('torch.FloatTensor')\n",
    "            self.testY = torch.from_numpy(np.asarray(labels,dtype=np.uint8)).type('torch.LongTensor')\n",
    "        else:\n",
    "            self.setTest(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.isTrain:\n",
    "            return len(self.trainX)\n",
    "        else:\n",
    "            return len(self.testX)\n",
    "\n",
    "    def setTest(self,testI):\n",
    "        if self.isVal:\n",
    "            d = self.data\n",
    "            l = self.labels\n",
    "            self.testX = torch.from_numpy(np.asarray(d,dtype=np.uint8)).type('torch.FloatTensor')\n",
    "            self.testY = torch.from_numpy(np.asarray(l,dtype=np.uint8)).type('torch.LongTensor')\n",
    "            return\n",
    "        d = np.empty((0,3,384,683),int)\n",
    "        l = []\n",
    "        for i in range(5):\n",
    "            if i != testI:\n",
    "                d = np.append(d,self.data[i],axis=0)\n",
    "                l += self.labels[i]\n",
    "            else:\n",
    "                self.testX = torch.from_numpy(np.asarray(data[i],dtype=np.uint8)).type('torch.FloatTensor')\n",
    "                self.testY = torch.from_numpy(np.asarray(labels[i],dtype=np.uint8)).type('torch.LongTensor')\n",
    "                \n",
    "        self.trainX = torch.from_numpy(np.asarray(d,dtype=np.uint8)).type('torch.FloatTensor')\n",
    "        self.trainY = torch.from_numpy(np.asarray(l,dtype=np.uint8)).type('torch.LongTensor')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.isTrain:\n",
    "            return {\"img\" : self.trainX[idx], \"label\": self.trainY[idx]}\n",
    "        else:\n",
    "            return {\"img\" : self.testX[idx], \"label\": self.testY[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model,crit,opt,x, y):\n",
    "    opt.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = crit(y_pred, y)\n",
    "    loss.backward()\n",
    "    opt.step() \n",
    "    return loss.item(),torch.sum(torch.max(y_pred, 1)[1] == y).item() / len(y)\n",
    "\n",
    "def train(model,crit,opt,dataload,disable=False):\n",
    "    # Training loop\n",
    "    dloss, dacc = 0,0\n",
    "    for s in tqdm(dataload,disable=disable):\n",
    "        x, y = s.values()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        lo, acc = train_batch(model,crit,opt,x,y)\n",
    "        dloss += lo\n",
    "        dacc += acc\n",
    "    dloss /= len(dataload)\n",
    "    dacc /= len(dataload)\n",
    "    return dloss,dacc\n",
    "\n",
    "def test_batch(model,crit,x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = crit(y_pred, y)\n",
    "    return loss.item(),torch.sum(torch.max(y_pred, 1)[1] == y).item() / len(y)\n",
    "\n",
    "def test(model,crit,dataload,disable=False):\n",
    "    # Training loop\n",
    "    dloss, dacc = 0,0\n",
    "    with torch.no_grad():\n",
    "        for s in tqdm(dataload,disable=disable):\n",
    "            x, y = s.values()\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            lo, acc = test_batch(model,crit,x,y)\n",
    "            dloss += lo\n",
    "            dacc += acc\n",
    "    dloss /= len(dataload)\n",
    "    dacc /= len(dataload)\n",
    "    return dloss,dacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = AutoUXDataset(data,labels,True)\n",
    "my_test = AutoUXDataset(data,labels,False)\n",
    "\n",
    "random.seed(4815162352)\n",
    "batch_sizes = [int(np.round(2 ** random.normalvariate(7,0.862519722))) for i in range(40)]\n",
    "weight_decays = [np.exp(random.normalvariate(0,0.5)) for i in range(40)]\n",
    "stats = []\n",
    "\n",
    "disable = False\n",
    "epochs = 50\n",
    "every_other = 10\n",
    "\n",
    "for i in range(len(batch_sizes)):\n",
    "    avedloss, avedacc, avevloss, avevacc = 0,0,0,0\n",
    "    batch_size = batch_sizes[i]\n",
    "    weight_decay = weight_decays[i]\n",
    "    for i in range(5):\n",
    "        torch.manual_seed(4815162342)\n",
    "        model = LogisticRegression().cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=3e-4,weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        dlosscurve = []\n",
    "        dacccurve = []\n",
    "        vlosscurve = []\n",
    "        vacccurve = []\n",
    "        print(\"Fold %d\" % (i + 1))\n",
    "        my_data.setTest(i)\n",
    "        my_test.setTest(i)\n",
    "\n",
    "        my_loader = DataLoader(my_data, batch_size=batch_size,\n",
    "                                shuffle=True)\n",
    "        my_test_loader = DataLoader(my_test, batch_size=batch_size,\n",
    "                                shuffle=True)\n",
    "        for j in range(epochs):\n",
    "            dloss, dacc = train(model,criterion,optimizer,my_loader,disable)\n",
    "            vloss, vacc = test(model,criterion,my_test_loader,disable)\n",
    "            dlosscurve.append(dloss)\n",
    "            dacccurve.append(dacc)\n",
    "            vlosscurve.append(vloss)\n",
    "            vacccurve.append(vacc)\n",
    "            if j % every_other == 0:\n",
    "                print(\"Epoch Train Loss: {:.6f}  Epoch Train Accuracy: {:.6f}  Epoch Test Loss: {:.6f}  Epoch Test Accuracy: {:.6f}\".format(dloss,dacc * 100,vloss,vacc * 100))\n",
    "        df = pd.DataFrame(data={\"train_loss\": dlosscurve, \"val_loss\": vlosscurve})\n",
    "        df.plot.line()\n",
    "        avedloss += dloss\n",
    "        avedacc += dacc\n",
    "        avevloss += vloss\n",
    "        avevacc += vacc\n",
    "    avedloss /= 5\n",
    "    avedacc /= 0.05\n",
    "    avevloss /= 5\n",
    "    avevacc /= 0.05\n",
    "    print(\"Average Train Loss: {:.6f}\\nAverage Train Accuracy: {:.6f}\\nAverage Test Loss: {:.6f}\\nAverage Test Accuracy: {:.6f}\\n\".format(avedloss,avedacc,avevloss,avevacc))\n",
    "    stats.append([avedloss,avedacc,avevloss,avevacc])\n",
    "\n",
    "for i in range(len(batch_sizes)):\n",
    "    print(\"{:.2f} & {:.2f} & {} \\\\\\\\ \\\\hline\".format(batch_sizes[i],weight_decays[i],\" & \".join([\"{:.2f}{}\".format(stats[i][j],\"\\\\%\" if j % 2 == 1 else \"\") for j in range(len(stats[i]))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  5.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 55.503880  Epoch Train Accuracy: 49.328737  Epoch Test Loss: 29.777473  Epoch Test Accuracy: 57.777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 0.178283  Epoch Train Accuracy: 94.485618  Epoch Test Loss: 1.539584  Epoch Test Accuracy: 84.444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 0.189565  Epoch Train Accuracy: 95.261671  Epoch Test Loss: 1.366446  Epoch Test Accuracy: 82.222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 0.853124  Epoch Train Accuracy: 87.142727  Epoch Test Loss: 0.937304  Epoch Test Accuracy: 82.222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 0.036957  Epoch Train Accuracy: 99.002217  Epoch Test Loss: 0.619807  Epoch Test Accuracy: 80.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  5.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00,  6.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.584163\n",
      "Validation Accuracy: 84.444444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 82\n",
    "weight_decay = 0.78\n",
    "disable = False\n",
    "epochs = 50\n",
    "every_other = 10\n",
    "\n",
    "# initializing training objects\n",
    "model = LogisticRegression().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4,weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# initializing dataset\n",
    "my_data = AutoUXDataset(data,labels,True)\n",
    "my_val = AutoUXDataset(dataVal,labelsVal,False,True)\n",
    "my_loader = DataLoader(my_data, batch_size=batch_size,shuffle=True)\n",
    "my_val_loader = DataLoader(my_val, batch_size=batch_size,shuffle=True)\n",
    "my_data.setTest(-1)\n",
    "my_val.setTest(-1)\n",
    "\n",
    "# initializing tracking variables\n",
    "dlosscurve = []\n",
    "dacccurve = []\n",
    "vlosscurve = []\n",
    "vacccurve = []\n",
    "\n",
    "# training loop\n",
    "for j in range(epochs):\n",
    "    dloss, dacc = train(model,criterion,optimizer,my_loader,disable)\n",
    "    vloss, vacc = test(model,criterion,my_val_loader,disable)\n",
    "    dlosscurve.append(dloss)\n",
    "    dacccurve.append(dacc)\n",
    "    vlosscurve.append(vloss)\n",
    "    vacccurve.append(vacc)\n",
    "    if j % every_other == 0:\n",
    "        print(\"Epoch Train Loss: {:.6f}  Epoch Train Accuracy: {:.6f}  Epoch Test Loss: {:.6f}  Epoch Test Accuracy: {:.6f}\".format(dloss,dacc * 100,vloss,vacc * 100))\n",
    "\n",
    "print(\"Train Loss: {:.6f}\\nAverage Train Accuracy: {:.6f}\\nValidation Loss: {:.6f}\\nValidation Accuracy: {:.6f}\\n\".format(dloss,dacc * 100,vloss,vacc * 100))\n",
    "print(\"{:.2f} & {:.2f}\\% & {:.2f} & {:.2f}\\% \\\\\\\\ \\\\hline\".format(dloss,dacc * 100,vloss, vacc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"train_loss\": dlosscurve, \"val_loss\": vlosscurve})\n",
    "df.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = np.array([])\n",
    "preds = np.array([])\n",
    "with torch.no_grad():\n",
    "    for s in tqdm(my_val,disable=disable):\n",
    "        x, y = s.values()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        y_pred = model(x).cpu()\n",
    "        answers = np.append(answers,y_pred[:,1])\n",
    "        preds = np.append(preds,torch.max(y_pred,1)[1])\n",
    "answers = np.exp(answers)\n",
    "print(answers)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logregoutprob.pt', 'wb') as f:\n",
    "    pickle.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "import itertools\n",
    "import pickle\n",
    "with open('logregout.pt', 'rb') as f:\n",
    "    y_pred = pickle.load(f)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "y_test = np.array([])\n",
    "\n",
    "for x in labels:\n",
    "    y_test = np.append(y_test,x)\n",
    "\n",
    "class_names = [\"bad site\",\"good site\"]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print(\"Accuracy: {:.4f}\".format(np.mean(y_test==y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "\n",
    "with open('AffonsoNetoutprobs.pt', 'rb') as f:\n",
    "    outvals = pickle.load(f)\n",
    "# print(outvals)\n",
    "ratings = []\n",
    "headers = []\n",
    "indices = []\n",
    "\n",
    "with open('ratingsave.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            headers = row\n",
    "            line_count += 1\n",
    "        else:\n",
    "            cur = []\n",
    "            for i in range(-3,0):\n",
    "                cur.append(row[i])\n",
    "            ratings.append(cur)\n",
    "            if cur[0] != \"#DIV/0!\":\n",
    "                indices.append(line_count - 1)\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "# print(ratings)\n",
    "\n",
    "_r = []\n",
    "_o = []\n",
    "for i in indices:\n",
    "    _r.append([float(r) for r in ratings[i]])\n",
    "    _o.append(outvals[i])\n",
    "ratings = np.array(_r)\n",
    "ratings = (ratings - np.ones(ratings.shape)) / 3.0\n",
    "outvals = np.array(_o)\n",
    "outvals = outvals.reshape((outvals.shape[0],1))\n",
    "# print(ratings)\n",
    "# print(ratings.shape,outvals.shape)\n",
    "ratings = np.append(ratings,outvals,axis = 1)\n",
    "# print(ratings.shape,outvals.shape)\n",
    "\n",
    "res = np.corrcoef(ratings.T)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
