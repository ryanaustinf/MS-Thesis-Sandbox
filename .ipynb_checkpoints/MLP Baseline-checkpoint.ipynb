{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import PIL\n",
    "from random import shuffle\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataconvall.pt', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "with open('labelsall.pt', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "    \n",
    "with open('datavalconvall.pt', 'rb') as f:\n",
    "    dataVal = pickle.load(f)\n",
    "    \n",
    "with open('labelsval.pt', 'rb') as f:\n",
    "    labelsVal = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(786816, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = X.view(X.size(0), -1)\n",
    "        out = self.dropout(torch.relu(self.fc1(out)))\n",
    "        out = self.dropout(torch.relu(self.fc2(out)))\n",
    "        out = self.dropout(torch.relu(self.fc3(out)))\n",
    "        out = torch.log_softmax(self.fc4(out), 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoUXDataset(Dataset):\n",
    "    def __init__(self, data, labels,isTrain=True,isVal=False):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.isTrain = isTrain\n",
    "        self.isVal = isVal\n",
    "        if isVal:\n",
    "            self.testX = torch.from_numpy(np.asarray(data,dtype=np.uint8)).type('torch.FloatTensor')\n",
    "            self.testY = torch.from_numpy(np.asarray(labels,dtype=np.uint8)).type('torch.LongTensor')\n",
    "        else:\n",
    "            self.setTest(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.isTrain:\n",
    "            return len(self.trainX)\n",
    "        else:\n",
    "            return len(self.testX)\n",
    "\n",
    "    def setTest(self,testI):\n",
    "        if self.isVal:\n",
    "            d = self.data\n",
    "            l = self.labels\n",
    "            self.testX = torch.from_numpy(np.asarray(d,dtype=np.uint8)).type('torch.FloatTensor')\n",
    "            self.testY = torch.from_numpy(np.asarray(l,dtype=np.uint8)).type('torch.LongTensor')\n",
    "            return\n",
    "        d = np.empty((0,3,384,683),int)\n",
    "        l = []\n",
    "        for i in range(5):\n",
    "            if i != testI:\n",
    "                d = np.append(d,self.data[i],axis=0)\n",
    "                l += self.labels[i]\n",
    "            else:\n",
    "                self.testX = torch.from_numpy(np.asarray(data[i],dtype=np.uint8)).type('torch.FloatTensor')\n",
    "                self.testY = torch.from_numpy(np.asarray(labels[i],dtype=np.uint8)).type('torch.LongTensor')\n",
    "                \n",
    "        self.trainX = torch.from_numpy(np.asarray(d,dtype=np.uint8)).type('torch.FloatTensor')\n",
    "        self.trainY = torch.from_numpy(np.asarray(l,dtype=np.uint8)).type('torch.LongTensor')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.isTrain:\n",
    "            return {\"img\" : self.trainX[idx], \"label\": self.trainY[idx]}\n",
    "        else:\n",
    "            return {\"img\" : self.testX[idx], \"label\": self.testY[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model,crit,opt,x, y):\n",
    "    opt.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = crit(y_pred, y)\n",
    "    loss.backward()\n",
    "    opt.step() \n",
    "    return loss.item(),torch.sum(torch.max(y_pred, 1)[1] == y).item() / len(y)\n",
    "\n",
    "def train(model,crit,opt,dataload,disable=False):\n",
    "    # Training loop\n",
    "    dloss, dacc = 0,0\n",
    "    for s in tqdm(dataload,disable=disable):\n",
    "        x, y = s.values()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        lo, acc = train_batch(model,crit,opt,x,y)\n",
    "        dloss += lo\n",
    "        dacc += acc\n",
    "    dloss /= len(dataload)\n",
    "    dacc /= len(dataload)\n",
    "    return dloss,dacc\n",
    "\n",
    "def test_batch(model,crit,x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = crit(y_pred, y)\n",
    "    return loss.item(),torch.sum(torch.max(y_pred, 1)[1] == y).item() / len(y)\n",
    "\n",
    "def test(model,crit,dataload,disable=False):\n",
    "    # Training loop\n",
    "    dloss, dacc = 0,0\n",
    "    with torch.no_grad():\n",
    "        for s in tqdm(dataload,disable=disable):\n",
    "            x, y = s.values()\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            lo, acc = test_batch(model,crit,x,y)\n",
    "            dloss += lo\n",
    "            dacc += acc\n",
    "    dloss /= len(dataload)\n",
    "    dacc /= len(dataload)\n",
    "    return dloss,dacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 10.030128  Epoch Train Accuracy: 48.466323  Epoch Test Loss: 8.132627  Epoch Test Accuracy: 58.333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 0.891649  Epoch Train Accuracy: 49.084084  Epoch Test Loss: 1.053401  Epoch Test Accuracy: 60.197698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Loss: 1.098733  Epoch Train Accuracy: 52.033462  Epoch Test Loss: 1.283054  Epoch Test Accuracy: 56.468969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.85it/s]\n",
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:01<00:00,  3.12it/s]"
     ]
    }
   ],
   "source": [
    "my_data = AutoUXDataset(data,labels,True)\n",
    "my_test = AutoUXDataset(data,labels,False)\n",
    "\n",
    "num_attempts = 10\n",
    "\n",
    "batch_sizes = [int(np.round(2 ** random.normalvariate(7,0.862519722))) for i in range(num_attempts)]\n",
    "hidden_dims = [int(np.round(2 ** random.normalvariate(7.5,0.562519722))) for i in range(num_attempts)]\n",
    "dropouts = [random.uniform(0,0.9) for i in range(num_attempts)]\n",
    "weight_decays = [np.exp(random.normalvariate(0,0.5)) for i in range(num_attempts)]\n",
    "stats = []\n",
    "\n",
    "disable = False\n",
    "epochs = 50\n",
    "every_other = 10\n",
    "\n",
    "for i in range(len(batch_sizes)):\n",
    "    avedloss, avedacc, avevloss, avevacc = 0,0,0,0\n",
    "    batch_size = batch_sizes[i]\n",
    "    weight_decay = weight_decays[i]\n",
    "    hidden_dim = hidden_dims[i]\n",
    "    dropout = dropouts[i]\n",
    "    for i in range(5):\n",
    "        model = MLP(hidden_dim=hidden_dim, dropout=dropout).cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=3e-4,weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        dlosscurve = []\n",
    "        dacccurve = []\n",
    "        vlosscurve = []\n",
    "        vacccurve = []\n",
    "        print(\"Fold %d\" % (i + 1))\n",
    "        my_data.setTest(i)\n",
    "        my_test.setTest(i)\n",
    "\n",
    "        my_loader = DataLoader(my_data, batch_size=batch_size,\n",
    "                                shuffle=True)\n",
    "        my_test_loader = DataLoader(my_test, batch_size=batch_size,\n",
    "                                shuffle=True)\n",
    "        for j in range(epochs):\n",
    "            dloss, dacc = train(model,criterion,optimizer,my_loader,disable)\n",
    "            vloss, vacc = test(model,criterion,my_test_loader,disable)\n",
    "            dlosscurve.append(dloss)\n",
    "            dacccurve.append(dacc)\n",
    "            vlosscurve.append(vloss)\n",
    "            vacccurve.append(vacc)\n",
    "            if j % every_other == 0:\n",
    "                print(\"Epoch Train Loss: {:.6f}  Epoch Train Accuracy: {:.6f}  Epoch Test Loss: {:.6f}  Epoch Test Accuracy: {:.6f}\".format(dloss,dacc * 100,vloss,vacc * 100))\n",
    "        df = pd.DataFrame(data={\"train_loss\": dlosscurve, \"val_loss\": vlosscurve})\n",
    "        df.plot.line()\n",
    "        avedloss += dloss\n",
    "        avedacc += dacc\n",
    "        avevloss += vloss\n",
    "        avevacc += vacc\n",
    "    avedloss /= 5\n",
    "    avedacc /= 0.05\n",
    "    avevloss /= 5\n",
    "    avevacc /= 0.05\n",
    "    print(\"Average Train Loss: {:.6f}\\nAverage Train Accuracy: {:.6f}\\nAverage Test Loss: {:.6f}\\nAverage Test Accuracy: {:.6f}\\n\".format(avedloss,avedacc,avevloss,avevacc))\n",
    "    stats.append([avedloss,avedacc,avevloss,avevacc])\n",
    "\n",
    "for i in range(len(batch_sizes)):\n",
    "    print(\"{:.2f} & {:.2f} & {} \\\\\\\\ \\\\hline\".format(batch_sizes[i],weight_decays[i],\" & \".join([\"{:.2f}{}\".format(stats[i][j],\"\\\\%\" if j % 2 == 1 else \"\") for j in range(len(stats[i]))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 82\n",
    "weight_decay = 0.78\n",
    "hidden_dim = \n",
    "dropout = \n",
    "disable = False\n",
    "epochs = 50\n",
    "every_other = 10\n",
    "\n",
    "# initializing training objects\n",
    "model = LogisticRegression().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4,weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# initializing dataset\n",
    "my_data = AutoUXDataset(data,labels,True)\n",
    "my_val = AutoUXDataset(dataVal,labelsVal,False,True)\n",
    "my_loader = DataLoader(my_data, batch_size=batch_size,shuffle=True)\n",
    "my_val_loader = DataLoader(my_val, batch_size=batch_size,shuffle=True)\n",
    "my_data.setTest(-1)\n",
    "my_val.setTest(-1)\n",
    "\n",
    "# initializing tracking variables\n",
    "dlosscurve = []\n",
    "dacccurve = []\n",
    "vlosscurve = []\n",
    "vacccurve = []\n",
    "\n",
    "# training loop\n",
    "for j in range(epochs):\n",
    "    dloss, dacc = train(model,criterion,optimizer,my_loader,disable)\n",
    "    vloss, vacc = test(model,criterion,my_val_loader,disable)\n",
    "    dlosscurve.append(dloss)\n",
    "    dacccurve.append(dacc)\n",
    "    vlosscurve.append(vloss)\n",
    "    vacccurve.append(vacc)\n",
    "    if j % every_other == 0:\n",
    "        print(\"Epoch Train Loss: {:.6f}  Epoch Train Accuracy: {:.6f}  Epoch Test Loss: {:.6f}  Epoch Test Accuracy: {:.6f}\".format(dloss,dacc * 100,vloss,vacc * 100))\n",
    "\n",
    "print(\"Train Loss: {:.6f}\\nTrain Accuracy: {:.6f}\\nValidation Loss: {:.6f}\\nValidation Accuracy: {:.6f}\\n\".format(dloss,dacc * 100,vloss,vacc * 100))\n",
    "print(\"{:.2f} & {:.2f}\\% & {:.2f} & {:.2f}\\% \\\\\\\\ \\\\hline\".format(dloss,dacc * 100,vloss, vacc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = np.array([])\n",
    "preds = np.array([])\n",
    "with torch.no_grad():\n",
    "    for s in tqdm(my_val,disable=disable):\n",
    "        x, y = s.values()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        y_pred = model(x).cpu()\n",
    "        answers = np.append(answers,y_pred[:,1])\n",
    "        preds = np.append(preds,torch.max(y_pred,1)[1])\n",
    "answers = np.exp(answers)\n",
    "print(answers)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mlpoutprob.pt', 'wb') as f:\n",
    "    pickle.dump(answers, f)\n",
    "with open('mlpout.pt','wb') as f:\n",
    "    pickle.dump(preds,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "import itertools\n",
    "import pickle\n",
    "with open('mlpout.pt', 'rb') as f:\n",
    "    y_pred = pickle.load(f)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "y_test = np.array([])\n",
    "\n",
    "for x in labels:\n",
    "    y_test = np.append(y_test,x)\n",
    "\n",
    "class_names = [\"bad site\",\"good site\"]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print(\"Accuracy: {:.4f}\".format(np.mean(y_test==y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
